# tzm.github.io
1. To summary what I have done during the internship. The title is A survey on Document
Layout Analysis. I focus on reading a paper called VSR, which is published on the ICDAR 2021. VSR won the first place In ICDAR2021 layout analysis recognition competition. 
I think reading this paper is meaningful for me to understand the field of Document Layout Analysis.

2. Here is the today presentation outline and we will start from the introduction and background.

3. Document layout analysis is the process of identifying and categorizing the regions of interest in the scanned image of a text document.
Document layout analysis is crucial for understanding document structures, which have many important applications, such as document retrieval, digitalization and editing.

4. So far, many deep learning models have been proposed on this task in both computer vision (CV) and natural language processing (NLP) communities.
Most of them consider either only visual features or only semantics features. 
NLP-based methods model layout analysis as a sequence labeling task and apply a bottom-up strategy.
CV-based methods treat layout analysis as object detection or segmentation task, and apply a top-down strategy.
However, information from both modalities could help recognize the document layout better. In this paper, the authors propose a unified framework VSR, both considering vision, semantics and component relation.

5. Here is the pipeline of the proposed VSR. It contains three parts: two-stream ConvNets, a multi-scale adaptive aggregation module and a relation module.
And This framework can be applied to both NLP-based and CV-based method. In this presentation, I will only focus on Computer vision side.
First, documents are fed into VSR in the form of images (vision) and text embedding maps (semantics at both character-level and sentence-level).
Then, visual and semantic features are extracted through a two-stream network, which are effectively combined later in a multi-scale adaptive aggregation module. 
Finally, a GNN(Graph Neural Network)-based relation module is incorporated to model relations between component candidates, and generate final results. 

6. Visual stream ConvNet directly takes document images as input and extracts multi-scale deep features using CNN backbones like ResNet, or ResNeXt.
multi-scale features maps (denoted by {V_2, V_3, V_4, V_5}) are extracted

7. The input of Semantic stream ConvNet is text embedding maps including both character and sentence level semantics.
We can use those two function to calculate the character level semantics map and sentence level semantics map.
In the character embedding, word embedding layer is used,
While in the sentence level semantics map generation, pretrained BERT is used.
The final semantics maps are constructed by applying layer Normaliation to the summation of character and sentence maps.
Finally, the same backbone as in visual stream is used to extract multi-scale features maps (denoted by {𝑆_2,𝑆_3, 𝑆_4, 𝑆_5}) 

8. The input of this Multi-scale Adaptive Aggregation Module is visual features {𝑉_2, 𝑉_3, 𝑉_4, 𝑉_5} and semantic features {𝑆_2,𝑆_3, 𝑆_4, 𝑆_5}.
The output is fused features {〖𝐹𝑀〗_2,〖𝐹𝑀〗_3, 〖𝐹𝑀〗_4, 〖𝐹𝑀〗_5}.
The feature fusion equation is shown here. Ami is a learned attention map.
We can simply understand this aggregation module as a attention module.

9. Given features maps, a standard object detection or segmentation model (like., Mask RCNN) can be used to generate component candidates in a document. Previous works directly take those predictions as final results. However, in the task of document layout analysis, strong relations exist between layout components. For example, bounding boxes of Paragraphs in the same column should be aligned; Table and Table Caption often appear together; there is almost no overlap between components. 
The authors thinks that such relations can be utilized to further refine predictions. GNN (graph neural network) can model component relations to refine prediction results.

10. In this paper, a document is treated as a graph G = (O, E), where O is the node set and E is the edge set and N is the nude number. 
Each node oj represents a component candidate generated by the object detection model previously, and each edge represents the relation between two component candidates. 
Here is the illustration of relation module. It captures relations between component candidates, and thus improves detection results (remove false Figure prediction, correct Table Caption label and adjust Paragraph coordinates). 

11. The key idea of the relation module is to update the hidden representations of each node by attending over its neighbors. 
With updated node features, we could predict its refined label and position coordinates.
The Graph Attention Network (GAT) layers are employed to learn the relations.

12. To utilize graph neural network, first, we need to define the nodes feature and edge relation of a graph from the component candidates.
For each node zj, we have the feature information fj extracted by backbone and position information bj derived by FasterRCNN. 
Therefore, each node feature zj can be constructed by the following equation. 
Where, fj is the regional features cropped and aligned from whole feature maps.
Bj is the corresponding coordinates of bbox. In faster rcnn, 4 float number, which are the x,y coordinates of topleft point and bottom right point, are used to represent a bbox.
Epos(bj) is the position embedding vectors of j-th node.
For edge connection relation, Since remote regions in a document may also bear close dependencies (e.g., a paragraph spans two columns), all regions may constitute a neighbor relationship. Thus, the document graph is treated as a fully-connected graph.
Why do we need position embedding?
As we know, the edge connection define the structure of a graph, as illustrated in here, if the edge connection is unchange, though those three graph looks different, they are the same graph in fact. 
In our task, we hope to keep the position information in the node feature, but the region feature fj do not contain position information due to the ROIalign operation.
Therefore, we need to use position embedding to encode the position information into the node feature.

13. What is positional encoding?
Position embedding or encoding is widely used since the population of Transformer.
Unlike RNN or LSTM, each word in a sentence simultaneously flows into the Transformer. The model itself doesn’t have any sense of position/order for each word. 
Consequently, there’s the need for a way to incorporate the order of the words into the model.
The position encoding function is shown here, which is only related to the length of a sentence and the dimension of the word vector. 
𝑝𝑜𝑠 is the position index and 𝑖 is the dimension index. 𝑑_𝑚𝑜𝑑𝑒𝑙 is the dimension of each word.
The output of positional embedding has the same shape with the word embedding vector.
Add up the word embedding and positional embedding vectors, we get the input of Transformer.
The situation in our task is similar, as the GNN do not consider the position information, position encoding could help to encode the position into node feature.
However, we got a problem in how to implement position embedding, because the paper did not give a clear explanation. I will talk about this problems later.

14. Once we define the node feature and edge of a graph, we could apply GNN to update the hidden representations of each node by attending over its neighbors. 
The Graph Attention Network (GAT) layers are used in this paper. The main concept of a Graph Attention layer is to update hidden representations hi of nodes into hi prime, as shown in this equation. 
In this equation, ℎ_𝑖 is the current node hidden representations and ℎ_𝑖′ is the current node hidden representations. 𝑊 is the learnable parameter.
and 𝛼_𝑖𝑗  specifies the importance of node 𝑗’s features to node 𝑖, which is a softmaxed scaled dot-product attention. 𝜎  Sigma is an activation function (relu, elu),
For single-head attention, ℎ_𝑖 prime is the activated summation of all weighted neighborhood nodes.
In the situation of multi-head attention, as illustrated in this figure, it shows the updating process of a three-heads attention by node 1 on its five neighbors. 
Different arrow styles and colors denote independent attention computations. The aggregated features from each head are concatenated or averaged to obtain final h1’.

15. Experiment in the paper
The authors evaluated the proposed method in serval benchmark dataset, here I just show the result and performance comparisons on PubLayNet dataset
VSR improves the performance of Faster RCNN and Mask RCNN on all classes and increases the final AP by 4.7%.
In ICDAR2021 layout analysis recognition competition, VSR also surpasses all participating teams and ranks first.

16. Another related experiment is the Ablation study on Relation module, which is to figure out the effectiveness of relation module by insert or remove it in faster RCNN and proposed VSR.
both unimodal Faster RCNN and VSR show consistent improvements after incorporating relation module, with 5.3% and 2.2% increase respectively, showing the benefits of introducing relations.

17. Here is a Qualitative comparison between VSR w/wo RM. For the visualization result, we could find introducing RM effectively removes duplicate predictions and provides more accurate detection results (both labels and coordinates).

18. Form this slide, I will introduce my implementation on this paper. Due to the limitation of time, I choose to focus on how to reproduce the effect of relation module during this internship. 
The main network I used is Faster-RCNN (the backbone is ResNet-50), which is implemented by Pytorch. 
PublayNet, which is a large document layout analysis dataset, was used for training and validation. 
The same evaluation metrics COCO mAP@IoU[0.5:0.95] was adopted. 
As the first attempt, I directly trained vanilla Faster-RCNN with PublayNet dataset.
Here is the training result and comparison with the result reported in the paper. My implementation result is close to the paper’s result and decent when considering the difference of the backbone. 

19. Here I will simply introduce the Pipeline of Faster-RCNN.
Firstly, the input image go through backbone network, we collected feature maps in different depth and make a pyramid feature list.
Then, RPN region proposal network will predict total N proposal in each different scales feature maps.
Sequentially, the ROI region of feature maps will be cropped and aligned according to the proposals. 
So far, we will get N regional feature maps with same shape.
For example, in my implementation, in this stage, we could get a tensor of 4 dimensions, whose shape is N, 256, 7,7.
Later, the N region features will be compressed by two liner layers. For each region, it will be represented by a 1024-dimension vector.
then, each vector will go though two Parallel linear layers to predict classes and bboxes regression for each roi region, respectively.
Finally, the low confidence objects will be filtered, and we got the final output.

20. In the paper, since there is limited detail description of where and how to deploy the Graph neural network, I have to make my implementation based on my own understanding.

21. Where and How to insert GAT into Faster RCNN? 
My first attempt is to make a parallel structure inside the ROI head of faster RCNN, shown in this slide.
In this case, the proposals predicted by Region proposal network will be treated as position information and the corresponding regional feature will be treated as node feature. 
This feature  is directly feed into GAT to update the node hidden feature, which is used to make class classification and bbox regression later.

22. Here is the experimental result of the implementation of the structure introduced in the last slide. We trained the network end-to-end 
From the table, we could observe that the output of original faster RCNN is close to the paper’s reporting result, which is reasonable 
However, the output of GNN is not better than the output of faster RCNN, which do not function as a refinement module.
I consider the following points may be the reason of failure:
	my conjectural network structure is wrong
	The hyperparameter of GNN is not good
	Lack of position embedding function

23. Another plan is to utilize final predicted bboxes to cut off new regional feature from pyramid feature. Then the regional feature will go through ROI align and be compressed by two linear layers. so far we could get the node feature represent corresponding to the final output of faster RCNN, which you could see the red arrow line in the slide. 
Then, we could get updated node feature representation here and using same linear layers to predict the refine classes and regression bboxes.
Finally, same postprocessing filter will be applied to remove the low confidence object.

24. I also tried to train my conjectural pipeline. Here is the experimental result. 
From the table, we found that the output of original faster RCNN is also in a reasonable range, but the output of GNN is totally 0.
To figure out why the output is 0, I checked the medium variant and find the most of outputs of GNN have low confidence, thus they are all filtered by postprocessing. This leads to no prediction in the final output of GNN branch, resulting all mAP to be 0.
Also, I consider the following points may be the reason of failure:
	my conjectural pipeline of network is wrong
	The hyperparameter of GNN is not good
	Wrong position embedding function
	Even I had debugged my code three days, there still a possibility that Bugs existing in my code.

